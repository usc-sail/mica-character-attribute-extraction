{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from narrative_understanding.cache import GetPage\n",
    "\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import urllib3\n",
    "import urllib\n",
    "import wget\n",
    "import datetime\n",
    "import collections\n",
    "import subprocess\n",
    "import pdftotext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(os.getenv(\"DATA_DIR\"), \"mica_narrative_understanding/data/scrape\")\n",
    "movie_scripts_dir = os.path.join(os.getenv(\"DATA_DIR\"), \"mica_narrative_understanding/data/movie_scripts\")\n",
    "\n",
    "def write_text(text: str, text_file: str):\n",
    "    \"\"\"Write `text` to the `text_file` text file.\"\"\"\n",
    "    with open(text_file, \"w\") as fw:\n",
    "        fw.write(text)\n",
    "\n",
    "\n",
    "def download_pdf(pdf_url: str, pdf_file: str) -> bool:\n",
    "    \"\"\"Download pdf from the `pdf_url` url (must end in .pdf or .PDF), save the document to the `pdf_file` path, \n",
    "    convert the pdf file to a text file, and save the text file to the same directory as the `pdf_file` path \n",
    "    with the same filename but with the .txt extension.\n",
    "\n",
    "    Args:\n",
    "        pdf_url (str) : url to a pdf document.\n",
    "        pdf_file (str) : filepath where pdf document will be saved.\n",
    "    \n",
    "    Returns:\n",
    "        success flag (bool) : True if the pdf is successfully downloaded and converted to a text file, else False.\n",
    "    \"\"\"\n",
    "    pdf_file = pdf_file if re.search(r\"\\.pdf$\", pdf_file) is not None else pdf_file + \".pdf\"\n",
    "    text_file = re.sub(r\"\\.pdf$\", \".txt\", pdf_file)\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(pdf_file, \"wb\") as fw:\n",
    "                fw.write(response.content)\n",
    "            with open(pdf_file, \"rb\") as fr:\n",
    "                pdf = pdftotext.PDF(fr)\n",
    "            text = \"\\n\\n\".join(pdf)\n",
    "            n_words = len(text.split())\n",
    "            assert n_words > 1000\n",
    "            write_text(text, text_file)\n",
    "        return True\n",
    "    except Exception:\n",
    "        if os.path.exists(pdf_file):\n",
    "            os.remove(pdf_file)\n",
    "        if os.path.exists(text_file):\n",
    "            os.remove(text_file)\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_movie_script(url: str, getpage: GetPage) -> tuple[str, int] | None:\n",
    "    \"\"\"Check if `url` links to a PDF script or find the script (PDF url or text) within the `url` webpage.\n",
    "    \n",
    "    Args:\n",
    "        url (str) : url \n",
    "        getpage (GetPage) : GetPage object to retrieve webpages\n",
    "\n",
    "    Returns:\n",
    "        str : a pdf url or script text\n",
    "        int : status code\n",
    "    \"\"\"\n",
    "    # url links to a PDF document\n",
    "    if url.endswith(\"pdf\") or url.endswith(\"PDF\"):\n",
    "        return url, 200\n",
    "\n",
    "    # retrieve page\n",
    "    soup_dict, status_code_dict = getpage(url, disable_progress_bar=True)\n",
    "\n",
    "    # url links to a TEXT document\n",
    "    if url.endswith(\"txt\") or url.endswith(\"TXT\"):\n",
    "        if url in soup_dict:\n",
    "            soup = soup_dict[url]\n",
    "            return soup.text, 200\n",
    "        elif url in status_code_dict:\n",
    "            return None, status_code_dict[url]\n",
    "\n",
    "    # script url links to scriptslug\n",
    "    elif re.search(\"scriptslug.com\", url) is not None:\n",
    "        if url in soup_dict:\n",
    "            soup = soup_dict[url]\n",
    "            for a_element in soup.find_all(\"a\"):\n",
    "                if a_element.text.lower().strip() == \"read the script\":\n",
    "                    pdf_url = a_element[\"href\"]\n",
    "                    if pdf_url.endswith(\"pdf\") or pdf_url.endswith(\"PDF\"):\n",
    "                        return pdf_url, 200\n",
    "        elif url in status_code_dict:\n",
    "            return None, status_code_dict[url]\n",
    "    \n",
    "    # script url links to imsdb html script\n",
    "    # script url links to dailyscript html script\n",
    "    # script url links to screenplays for you\n",
    "    elif re.search(r\"((imsdb)|(dailyscript)|(horrorlair))\\.com.*\\.html?$\", url) or (\n",
    "         re.search(\"sfy.ru\", url) is not None):\n",
    "        if url in soup_dict:\n",
    "            soup = soup_dict[url]\n",
    "            preformatted_element = soup.find(\"pre\")\n",
    "            if preformatted_element is not None:\n",
    "                return preformatted_element.text, 200\n",
    "        elif url in status_code_dict:\n",
    "            return None, status_code_dict[url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed Scripts On Screen index page\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://scripts-onscreen.com/movie-script-index/numeric-movie-script-index/: 100%|██████████| 27/27 [00:01<00:00, 22.14url/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved Scripts on Screen movie link category pages\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://scripts-onscreen.com/movie/99-homes-script-links/: 100%|██████████| 11059/11059 [02:33<00:00, 72.26url/s]                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 9349 movie pages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scripts on screen url and current date\n",
    "scripts_on_screen_url = \"https://scripts-onscreen.com/movie-script-index/\"\n",
    "date_accessed = datetime.datetime.now().strftime(\"%b %d %Y\")\n",
    "\n",
    "# initialize GetPage object to retrieve, parse, and cache webpages\n",
    "getpage = GetPage(cache_dir)\n",
    "\n",
    "# get scripts on screen index page and find the category pages of movie links\n",
    "parsed_pages, _ = getpage(scripts_on_screen_url, disable_progress_bar=True)\n",
    "index_soup = parsed_pages[scripts_on_screen_url]\n",
    "index_links_soup = index_soup.find(\"div\", attrs={\"class\":\"soslinks\"})\n",
    "links = [a_element[\"href\"] for a_element in index_links_soup.find_all(\"a\")]\n",
    "print(\"parsed Scripts On Screen index page\\n\")\n",
    "\n",
    "# get the category pages and find the movie links\n",
    "link_soups, _ = getpage(*links)\n",
    "movie_links = []\n",
    "for link_soup in link_soups.values():\n",
    "    list_elements = link_soup.find(\"div\", {\"class\": \"sosindex\"}).find_all(\"li\")\n",
    "    for list_element in list_elements:\n",
    "        movie_link = \"https://scripts-onscreen.com\" + list_element.find(\"a\")[\"href\"]\n",
    "        movie_links.append(movie_link)\n",
    "print(\"retrieved Scripts on Screen movie link category pages\\n\")\n",
    "\n",
    "# get the movie pages\n",
    "movie_pages, _ = getpage(*movie_links)\n",
    "print(f\"retrieved {len(movie_pages)} movie pages\\n\")\n",
    "\n",
    "# initialize the movie scripts index\n",
    "movie_scripts_directory = movie_scripts_dir\n",
    "movie_scripts_index_file = os.path.join(movie_scripts_directory, \"index.csv\")\n",
    "movie_scripts_index = {}\n",
    "next_file = 0\n",
    "\n",
    "# populate the movie scripts index with existing entries\n",
    "# the index is a mapping from script url to filename (without the extension), download date, IMDB id, MOVIEDB id,\n",
    "# and the synopsis on scripts on screen website\n",
    "if os.path.exists(movie_scripts_index_file):\n",
    "    movie_index_df = pd.read_csv(movie_scripts_index_file, index_col=None)\n",
    "    for _, row in movie_index_df.iterrows():\n",
    "        movie_scripts_index[row[\"url\"]] = (row[\"file\"], row[\"date\"], row[\"imdb_id\"], row[\"moviedb_id\"], \n",
    "                                        row[\"script_on_screen_synopsis\"])\n",
    "        next_file = max(next_file, row[\"file\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://web.archive.org/web/20180713235209/http://www.joblo.com:80/scripts/The%20A-Team.pdf                                                            failed to download\n",
      "https://www.docdroid.net/2GJdbnu/about-time-script-richard-curtis-pdf                                                                                  failed to download\n",
      "http://www.dailyscript.com/scripts/ace_ventura_shoot.html                                                                                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://drive.google.com/file/d/15FGH5Nl2QXcMEJHI9XaGXjm2lC2cjJbH/view                                                                                \n",
      "https://www.docdroid.net/muJitYr/ad-astra.pdf#page=2                                                                                                  \n",
      "http://www.beingcharliekaufman.com/index.php/scripts-writing/adaptation-2nd-draft-24-sept-1999/download                                               \n",
      "http://www.beingcharliekaufman.com/index.php/scripts-writing/adaptation-2nd-draft-revised-21-nov-2000/download                                        \n",
      "https://assets.scriptslug.com/live/pdf/scripts/the-adjustment-bureau-2011.pdf                                                                          failed to download\n",
      "http://www.screenplaydb.com/film/scripts/adventureland20070805/                                                                                       \n",
      "https://thescriptsavant.com/movies/The_Adventures_Of_TinTin.pdf                                                                                        failed to download\n",
      "http://www.dailyscript.com/scripts/an_affair_to_remember.pdf                                                                                           failed to download\n",
      "https://web.archive.org/web/20181222165831/http://www.scripthollywood.com:80/sitebuildercontent/sitebuilderfiles/afterthetruth.pdf                     failed to download\n",
      "http://www.screenplaydb.com/film/scripts/afterlife20081017/                                                                                           \n",
      "http://www.dailyscript.com/scripts/Alfie.pdf                                                                                                           failed to download\n",
      "https://assets.scriptslug.com/live/pdf/scripts/alien-1979.pdf                                                                                          failed to download\n",
      "https://web.archive.org/web/20161008041307/http://www.beckerfilms.com/AlienApocalypse-p1.htm                                                          \n",
      "https://assets.scriptslug.com/live/pdf/scripts/alien-3-1992.pdf                                                                                        failed to download\n",
      "http://www.dailyscript.com/scripts/AVP-FinalProduction.RTF                                                                                            \n",
      "https://assets.scriptslug.com/live/pdf/scripts/alien-vs-predator-requiem-2007.pdf                                                                      failed to download\n",
      "https://thescriptsavant.com/movies/All_That_Jazz.pdf                                                                                                   failed to download\n",
      "https://mega.nz/file/r0VwCDqb#JDAbuUdUK8C4p5khRWKapTgGH7de4e1zxS4besiZsSE                                                                             \n",
      "http://www.horrorlair.com/movies/alone_in_the_dark.html                                                                                               \n",
      "https://assets.scriptslug.com/live/pdf/scripts/altered-states-1980.pdf                                                                                 failed to download\n",
      "https://web.archive.org/web/20180616142448/http://www.screenplays-online.de:80/screenplay/6                                                           \n",
      "https://drive.google.com/file/d/10-3NhxQUtwFfcE60gY7khWe-9fPIX-IS/edit                                                                                \n",
      "http://www.dailyscript.com/scripts/AmericanBeauty_final.html                                                                                          \n",
      "https://drive.google.com/file/d/105uxNKnC8lyZ3PD2FJhzUrS6-SDrPSCg/view                                                                                \n",
      "https://web.archive.org/web/20040610183648/http://www.melsdrive-in.com/script.html                                                                    \n",
      "https://secure.sonypictures.com/movies/academy/media/americanhustle-screenplay.pdf                                                                     failed to download\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPConnectionPool(host='content.cdlib.org', port=8088): Max retries exceeded with url: /xtf/view?docId=ft7b69p14j&chunk.id=d0e9232&toc.depth=1&toc.id=&brand=ucpress&query=riskin&html.parser (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f02650708e0>, 'Connection to content.cdlib.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[39mraise\u001b[39;00m socket\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mgetaddrinfo returns an empty list\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[39m.\u001b[39;49mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connectionpool.py:398\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m         conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    400\u001b[0m \u001b[39m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connection.py:239\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    238\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 239\u001b[0m \u001b[39msuper\u001b[39;49m(HTTPConnection, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/http/client.py:1282\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/http/client.py:1328\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/http/client.py:1277\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1277\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1037\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m \n\u001b[1;32m   1041\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/http/client.py:975\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 975\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    976\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPConnection object at 0x7f02650708e0>, 'Connection to content.cdlib.org timed out. (connect timeout=None)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    502\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    788\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    789\u001b[0m )\n\u001b[1;32m    790\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mif\u001b[39;00m new_retry\u001b[39m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[39mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='content.cdlib.org', port=8088): Max retries exceeded with url: /xtf/view?docId=ft7b69p14j&chunk.id=d0e9232&toc.depth=1&toc.id=&brand=ucpress&query=riskin&html.parser (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f02650708e0>, 'Connection to content.cdlib.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m script_url_to_pdf_url, script_url_to_text \u001b[39m=\u001b[39m {}, {}\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m urls:\n\u001b[0;32m---> 43\u001b[0m     response \u001b[39m=\u001b[39m get_movie_script(url, getpage)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m         \u001b[39mif\u001b[39;00m response[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "Cell \u001b[0;32mIn[12], line 60\u001b[0m, in \u001b[0;36mget_movie_script\u001b[0;34m(url, getpage)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m url, \u001b[39m200\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[39m# retrieve page\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m soup_dict, status_code_dict \u001b[39m=\u001b[39m getpage(url, disable_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m \u001b[39m# url links to a TEXT document\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m url\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mtxt\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m url\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mTXT\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/proj/sbaruah/narrative_understanding/cache.py:52\u001b[0m, in \u001b[0;36mGetPage.__call__\u001b[0;34m(self, disable_progress_bar, *urls)\u001b[0m\n\u001b[1;32m     50\u001b[0m         status_codes[url] \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, \u001b[39m\"\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m199\u001b[39m \u001b[39m<\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m     54\u001b[0m         pages[url] \u001b[39m=\u001b[39m bs4\u001b[39m.\u001b[39mBeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/proj/sbaruah/anaconda3/envs/story/lib/python3.10/site-packages/requests/adapters.py:553\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    551\u001b[0m     \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, NewConnectionError):\n\u001b[0;32m--> 553\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectTimeout(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ResponseError):\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m RetryError(e, request\u001b[39m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPConnectionPool(host='content.cdlib.org', port=8088): Max retries exceeded with url: /xtf/view?docId=ft7b69p14j&chunk.id=d0e9232&toc.depth=1&toc.id=&brand=ucpress&query=riskin&html.parser (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f02650708e0>, 'Connection to content.cdlib.org timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "error_urls = []\n",
    "\n",
    "# loop over each movie page\n",
    "for i, movie_page in enumerate(movie_pages.values()):\n",
    "    main_div_soup = movie_page.find(\"div\", {\"class\": \"main_div\"})\n",
    "    synopsis, imdb_id, moviedb_id = None, None, None\n",
    "\n",
    "    # get scripts on screen synopsis\n",
    "    movie_synopsis_soups = [movie_prop_soup \n",
    "                            for movie_prop_soup in main_div_soup.find_all(\"div\", {\"class\": \"movie-prop\"}) \n",
    "                                if movie_prop_soup.text.startswith(\"Script Synopsis\")]\n",
    "    if movie_synopsis_soups:\n",
    "        synopsis = re.sub(f\"^Script Synopsis:\", \"\", movie_synopsis_soups[0].text).strip()\n",
    "    \n",
    "    # get script urls\n",
    "    movie_links_soup = main_div_soup.find(\"div\", {\"class\": \"movie-links\"})\n",
    "    urls = []\n",
    "    if movie_links_soup is not None:\n",
    "        list_elements = movie_links_soup.find_all(\"li\")\n",
    "        \n",
    "        # loop over each link\n",
    "        # find imdb and moviedb id\n",
    "        # find all script links that are not paid and does not contain the word 'Transcript' in the text\n",
    "        for list_element in list_elements:\n",
    "            url = list_element.find(\"a\")[\"href\"]\n",
    "            text = list_element.text\n",
    "            imdb_match = re.search(r\"\\(\\s*(tt\\d+)\\s*\\)\", text)\n",
    "            moviedb_match = re.search(r\"\\(\\s*(\\d+)\\s*\\)\", text)\n",
    "            if \"IMDb\" in text and imdb_match is not None:\n",
    "                imdb_id = imdb_match.group(1)\n",
    "            elif \"TheMovieDB.org\" in text and moviedb_match is not None:\n",
    "                moviedb_id = moviedb_match.group(1)\n",
    "            elif \"$\" not in text and \"Transcript\" not in text and url not in movie_scripts_index:\n",
    "                urls.append(url)\n",
    "\n",
    "    # get pdf urls or scrape the texts\n",
    "    # script url is the url on the movie page of scripts on screen used as index\n",
    "    # script url might not always be the same as the pdf url, for cases where more webpages needs to be retrieved\n",
    "    # to find the actual url to the movie script\n",
    "    script_url_to_pdf_url, script_url_to_text = {}, {}\n",
    "    for url in urls:\n",
    "        response = get_movie_script(url, getpage)\n",
    "        if response is not None:\n",
    "            if response[1] == 200:\n",
    "                if re.match(r\"http\", response[0]) is not None:\n",
    "                    script_url_to_pdf_url[url] = response[0]\n",
    "                else:\n",
    "                    script_url_to_text[url] = response[0]\n",
    "        else:\n",
    "            print(f\"{url:150s}\")\n",
    "            error_urls.append(url)\n",
    "    \n",
    "    # download pdfs\n",
    "    for script_url, pdf_url in script_url_to_pdf_url.items():\n",
    "        pdf_file = os.path.join(movie_scripts_directory, f\"{next_file}.pdf\")\n",
    "        if download_pdf(pdf_url, pdf_file):\n",
    "            movie_scripts_index[script_url] = (next_file, date_accessed, imdb_id, moviedb_id, synopsis)\n",
    "            next_file += 1\n",
    "        else:\n",
    "            print(f\"{pdf_url:150s} failed to download\")\n",
    "            error_urls.append(pdf_url)\n",
    "    \n",
    "    # write texts\n",
    "    for script_url, text in script_url_to_text.items():\n",
    "        movie_scripts_index[script_url] = (next_file, date_accessed, imdb_id, moviedb_id, synopsis)\n",
    "        text_file = os.path.join(movie_scripts_directory, f\"{next_file}.txt\")\n",
    "        next_file += 1\n",
    "        write_text(text, text_file)\n",
    "\n",
    "    # write movie scripts index and cache index periodically\n",
    "    if (i + 1) % 10 == 0 or i == len(movie_pages) - 1:\n",
    "        rows = []\n",
    "        for url, (file_id, date, imdb_id, moviedb_id, synopsis) in movie_scripts_index.items():\n",
    "            row = [url, file_id, date, imdb_id, moviedb_id, synopsis]\n",
    "            rows.append(row)\n",
    "        movie_index_df = pd.DataFrame(rows, columns=[\"url\", \"file\", \"date\", \"imdb_id\", \"moviedb_id\", \n",
    "                                                        \"script_on_screen_synopsis\"])\n",
    "        movie_index_df.to_csv(movie_scripts_index_file, index=False)\n",
    "        getpage._write_index()\n",
    "    \n",
    "    if len(error_urls) >= n:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "story",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8bce503439c6eaa153a35595c325f41107670a7162bb504f0fb0483c0a10a87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
